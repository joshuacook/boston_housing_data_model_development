{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language and libraries\n",
    "\n",
    "For this project, you will need to have the following software installed:\n",
    "\n",
    "Python 2.7\n",
    "NumPy\n",
    "scikit-learn\n",
    "Template code\n",
    "\n",
    "Download boston_housing.zip, unzip it and open the boston_housing.py template file. Follow the instructions to complete each step, and answer the questions in your report.\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Housing\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "Report in PDF format\n",
    "Fully implemented Boston Housing Python code as boston_housing.py\n",
    "You can package these two files as a single zip and submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n",
      "/Users/joshuacook/src/scikit/scikit-learn/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/joshuacook/src/scikit/scikit-learn/sklearn/grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%run \"../src/boston_housing.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "city_data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Report Structure\n",
    "\n",
    "### 1) Statistical Analysis and Data Exploration\n",
    "\n",
    "- Number of data points (houses)?\n",
    "- Number of features?\n",
    "- Minimum and maximum housing prices?\n",
    "- Mean and median Boston housing prices?\n",
    "- Standard deviation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of data points in this set is 506.\n",
      "The number of features in this set is 13.\n",
      "The minimum value of the target feature, housing price, is 5.\n",
      "The maximum value of the target feature, housing price, is 50.\n",
      "The mean value of the target feature, housing price, is 22.5328.\n",
      "The median value of the target feature, housing price, is 21.2000.\n",
      "The standard deviation of the target feature, housing price, is 9.1880.\n"
     ]
    }
   ],
   "source": [
    "# Explore the data\n",
    "explore_city_data(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training/Test dataset split\n",
    "X_train, y_train, X_test, y_test = split_data(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning Curve Graphs\n",
    "max_depths = [1,2,3,4,5,6,7,8,9,10]\n",
    "for max_depth in max_depths:\n",
    "    learning_curve(max_depth, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Complexity Graph\n",
    "model_complexity(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the features and labels from the Boston housing data\n",
    "X, y = city_data.data, city_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup a Decision Tree Regressor\n",
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}\n",
    "\n",
    "    ###################################\n",
    "    ### Step 4. YOUR CODE GOES HERE ###\n",
    "    ###################################\n",
    "\n",
    "    # 1. Find the best performance metric\n",
    "    # should be the same as your performance_metric procedure\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html\n",
    "    # wraps scoring functions to be used in GridSearchCV\n",
    "    performance_metric_scorer = make_scorer(performance_metric)\n",
    "\n",
    "\n",
    "    # 2. Use gridearch to fine tune the Decision Tree Regressor and find the best model\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV\n",
    "    reg = GridSearchCV(regressor, parameters, scoring=performance_metric_scorer)\n",
    "\n",
    "    # Fit the learner to the training data\n",
    "    print \"Final Model: \"\n",
    "    print reg.fit(X, y)\n",
    "\n",
    "    # Use the model to predict the output of a particular sample\n",
    "    x = [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]\n",
    "    y = reg.predict(x)\n",
    "    print \"House: \" + str(x)\n",
    "    print \"Prediction: \" + str(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune and predict Model\n",
    "fit_predict_model(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune and predict Model\n",
    "fit_predict_model(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune and predict Model\n",
    "fit_predict_model(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune and predict Model\n",
    "fit_predict_model(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune and predict Model\n",
    "fit_predict_model(city_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Evaluating Model Performance\n",
    "\n",
    "- *Which measure of model performance is best to use for predicting Boston housing data and analyzing the errors?* \n",
    "\n",
    "  Scikit-learn has several built-in tools for quantifying the quality of prediction models. In this project, our target feature is a continuous variable, make the prediction of the variable a regression rather than classification problem. The following metrics are listed in the reference section:\n",
    "    \n",
    "    - [`metrics.explained_variance_score(y_true, y_pred)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score)\tExplained variance regression score function\n",
    "    - [`metrics.mean_absolute_error(y_true, y_pred)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)\tMean absolute error regression loss\n",
    "    - [`metrics.mean_squared_error(y_true, y_pred[, ...])`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)\tMean squared error regression loss\n",
    "    - [`metrics.median_absolute_error(y_true, y_pred)`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.median_absolute_error.html#sklearn.metrics.median_absolute_error)\tMedian absolute error regression loss\n",
    "    - [`metrics.r2_score(y_true, y_pred[, ...])`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)\tR^2 (coefficient of determination) regression score function.  \n",
    "   \n",
    "  With little exposure to any of these, I chose to familiarize myself with the two metrics treated in the support material, mean absolute error and mean squared error, doing a little research into the topic. I found the following:\n",
    "  \n",
    "  > Both the root mean square error (RMSE) and the mean absolute error (MAE) are regularly employed in model evaluation studies. [^Chai]\n",
    "  \n",
    "[^Chai]: Chai, Tianfeng, and Roland R. Draxler. \"Root mean square error (RMSE) or mean absolute error (MAE)?â€“Arguments against avoiding RMSE in the literature.\" Geoscientific Model Development 7.3 (2014): 1247-1250.\n",
    "\n",
    "  > The mean absolute error ... is less sensitive to the occasional very large error because it does not square the errors in the calculation. [^rnau]\n",
    "  \n",
    "  > The root mean squared error is more sensitive than other measures to the occasional large error: the squaring process gives disproportionate weight to very large errors. If an occasional large error is not a problem in your decision situation (e.g., if the true cost of an error is roughly proportional to the size of the error, not the square of the error), then the MAE or MAPE may be a more relevant criterion.\n",
    "    \n",
    "  > There is no absolute criterion for a \"good\" value of RMSE or MAE: it depends on the units in which the variable is measured and on the degree of forecasting accuracy, as measured in those units, which is sought in a particular application. Depending on the choice of units, the RMSE or MAE of your best model could be measured in zillions or one-zillionths. It makes no sense to say \"the model is good (bad) because the root mean squared error is less (greater) than x\", unless you are referring to a specific degree of accuracy that is relevant to your forecasting application. \n",
    "  \n",
    "[^rnau]: http://people.duke.edu/~rnau/compare.htm\n",
    "\n",
    "[^Hyndmand_Koehler]\n",
    "[^Hyndmand_Koehler]: Hyndman, Rob J., and Anne B. Koehler. \"Another look at measures of forecast accuracy.\" International journal of forecasting 22.4 (2006): 679-688.\n",
    "  \n",
    "- Why do you think this measurement most appropriate? \n",
    "\n",
    "  *According to [^rnau]* \n",
    "  \n",
    "   > If an occasional large error is not a problem in your decision situation (e.g., if the true cost of an error is roughly proportional to the size of the error, not the square of the error), then the MAE or MAPE may be a more relevant criterion.\n",
    "   \n",
    "   This seems to fit with the data we are looking at so I chose to go with the Mean Absolute Error as my performance metric. \n",
    "   \n",
    "\n",
    "- *Why might the other measurements not be appropriate here?*  \n",
    "\n",
    "  Explained variance is primarily aimed at principal component analysis. Of course, we shouldn't use classfication or clustering metrics on this problem, a regression.\n",
    "\n",
    "- *Why is it important to split the Boston housing data into training and testing data? What happens if you do not do this?*\n",
    "\n",
    "  We use training data to form the model and testing data to assess the validity of the model. If we did not reserve some of our data for testing then when we used data to test the validity, this data would have been used in the formation of the model. We would expect any data that had been used to form the model to fit the model well. With test data we can see how well data not used to form the model fits with the model that was formed.\n",
    "\n",
    "- *What does grid search do and why might you want to use it?*\n",
    "\n",
    "  Grid search takes a given model and a set of parameters and compares the performance of the model against each of the parameters in sequence. The goal is to fine-tune the model against the parameters, finding the parameter against which the model performs best. \n",
    "\n",
    "- *Why is cross validation useful and why might we use it with grid search?*\n",
    "\n",
    "  Cross validation is a process by which a training set of data is split into $K$ sets. A train-test analysis is then performed against these sets reserving a different individual set as the test set in each pass. Using cross validation with grid search effectively multiplies the number of validation analyses performed against our model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Analyzing Model Performance\n",
    "\n",
    "- *Look at all learning curve graphs provided. What is the general trend of training and testing error as training size increases?*\n",
    "\n",
    "  Generally speaking, both training and testing error rapidly decrease as $n\\to50$ and then slowly decrease as the remaining data points are added to the analysis.\n",
    "  \n",
    "- *Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?*\n",
    "\n",
    "  The error is clearly diminished for a depth of 10, though it is unclear whether a depth of 10 is significantly better than say a depth of 5. A depth of 5 is certainly better than a depth of 1. \n",
    "\n",
    "- *Look at the model complexity graph. How do the training and test error relate to increasing model complexity? Based on this relationship, which model (max depth) best generalizes the dataset and why?*\n",
    " \n",
    "  It would appear that the test data error has a minimum near a depth of 6. Based upon this the error would decrease as depth increases until the depth of 6, after which it would increase, suffering from over fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Model Prediction\n",
    "\n",
    "- *Model makes predicted housing price with detailed model parameters (max depth). Note due to the small randomization of the code it is recommended to run the program several times to identify the most common/reasoable price/model complexity.*\n",
    "- *Compare prediction to earlier statistics and make a case if you think it is a valid model.*\n",
    "\n",
    "  The final prediction is run against this data point and returns the shown value.\n",
    "  \n",
    "  ```\n",
    "  House: [11.95, 0.0, 18.1, 0, 0.659, 5.609, 90.0, 1.385, 24, 680.0, 20.2, 332.09, 12.13]\n",
    "  Prediction: [ 21.62974359]\n",
    "  ```\n",
    "  \n",
    "  It is difficult to look at the original data set and speculate as the validity of this answer. As humans, we have little intuition for an 11-dimensional domain. That said, the process throuch which we generated this response was a reasonable one, involving both cross-validation and grid search. As such, I have a high degree of faith in this response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
